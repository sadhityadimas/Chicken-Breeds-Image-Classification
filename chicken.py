# -*- coding: utf-8 -*-
"""Chicken.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kGfQli0bO0oF8awmiK-48oxA3tXT8aeS
"""

#install required library first
! pip install -q kaggle 
from google.colab import files
files.upload() #upload api keys from kaggle, use your own api keys

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets list

# Download dataset from kaggle
! kaggle datasets download -d abdalnassir/chicken-breeds

! mkdir chicken
! unzip /content/chicken-breeds.zip -d chicken

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
from google.colab import files
import zipfile,os 
import pandas as pd
import pickle as pkl

!pip install split_folders tqdm #for dataset train test split 
!pip install seedir #for directory treeview 
import splitfolders #for easier dataset splitting  
import seedir as sd #for directory tree viewer

#target directory
target_dir = '/content/chicken/Chicken Breeds'

sd.seedir(target_dir, style='lines', itemlimit=4, depthlimit=3)

#count total files in train and validation set 
split_path = '/content/chicken/Chicken Breeds/'
folder_names = ['Chick', 'Speckled Sussex', 'American Gamefowl', 'Sapphire Gem', 'Wyandotte']

original_files ={}
train_files = {}
val_files = {}

for i in folder_names:
    train_files[i] = len(os.listdir(split_path+'training/'+i))
    val_files[i] = len(os.listdir(split_path+'validation/'+i))

    
#convert to dataframe for easier comprehension
result_split = pd.DataFrame()
result_split = result_split.append(train_files, ignore_index=True)
result_split = result_split.append(val_files, ignore_index=True)
result_split['total'] = result_split.sum(axis=1)
result_split['type'] = ['train', 'val']
result_split = result_split[['type', 'Chick', 'Speckled Sussex', 'American Gamefowl', 'Sapphire Gem', 'Wyandotte', 'total']]
print(result_split)
print(result_split.to_markdown(index=False))

#Display random images
import random

#function for displaying 1 random image and image shape
def one_random_image(target_path, target_class): 
  target_fold = target_path + target_class
  random_image = random.sample(os.listdir(target_fold), 1)
  image = mpimg.imread(target_fold+'/'+random_image[0])
  plt.imshow(image)
  plt.title(target_class)
  plt.axis('off');
  print(f"Image shape {image.shape}")

  return image

#Function for displaying group of random images and shape
def group_random_images( target_path, figure_size=(20, 10), group=20):
  plt.figure(figsize=figure_size)
  for i in range(group):
    plt.subplot(4, 5, i+1)
    class_name = random.choice(['Chick', 'Speckled Sussex', 'American Gamefowl', 'Sapphire Gem', 'Wyandotte'])
    image = one_random_image(target_path=target_path, target_class=class_name)
  
group_random_images = group_random_images(target_path='/content/chicken/Chicken Breeds/training/')

## saving train and validation path into variable
train_set = '/content/chicken/Chicken Breeds/training'
val_set = '/content/chicken/Chicken Breeds/validation'

#image augmentation
#training set rescale and augmenting
train_aug = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=30,
                    horizontal_flip=True,
                    zoom_range=0.2,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

#only rescale the validation set, dont do the augmentation step for validation set, it might cause bias
valid_aug = ImageDataGenerator(rescale=1./255)

#train and validation data generator
#batch size 64
#class mode is categorical since we have 5 classes
train_generator = train_aug.flow_from_directory(
        train_set,  
        target_size=(128, 128),  
        class_mode='categorical')

validation_generator = valid_aug.flow_from_directory(
        val_set, 
        target_size=(128, 128), 
        class_mode='categorical')

#building model 
#using 2 hidden layer 
#using softmax in output layer instead of sigmoid because there are 5 outcomes
model = keras.Sequential([
    layers.Conv2D(32, (3,3), activation = 'relu', input_shape= (128,128,3)),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(64,(3,3), activation= 'relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Conv2D(128,(3,3), activation= 'relu'),
    layers.MaxPooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dropout(0.5),
    layers.Dense(128, activation= 'relu'),
    layers.Dense(5, activation= 'softmax') 
])
model.summary()

# Adding lose function and optimizer
#categorical since we have 5 classes
#using adam 
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# callback
class myCallback(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy') >= 0.85):
      self.model.stop_training = True
      print("\nValidation Accuracy of the model >= 85%")
early_stop = myCallback()

# Training the model
history = model.fit(
    train_generator,
    epochs=40, 
    validation_data = validation_generator,
    verbose = 1,
    callbacks=[early_stop]
)

#loss 
history_df = pd.DataFrame(history.history)
history_df.loc[1:, ['loss', 'val_loss']].plot()
plt.show()

#accuracy 
history_df.loc[1:, ['accuracy', 'val_accuracy']].plot()
plt.show()

#Best validation loss and accuracy score
print(("Best Validation Loss: {:0.2f}" +"\nBest Validation Accuracy: {:0.2f}").format(history_df['val_loss'].min(), history_df['val_accuracy'].max()))

# Save the trained model 
model.save('my_model.h5', include_optimizer=False)

# check if the saved model works
# Recreate the exact same model, including its weights and the optimizer
new_model = tf.keras.models.load_model('my_model.h5')

# Show the model architecture
new_model.summary()

#try to predict
upload_file = files.upload()
 
for file_name in upload_file.keys():
 
  # predicting images
  file_path = file_name
  img = image.load_img(file_path, target_size=(128,128))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  predict = new_model.predict(images, batch_size=10)
  print(file_name)
  predict

#try the prediction
if predict[0][0] == 1.:
  print('American Gamefowl')
elif predict[0][1] == 1.:
  print('Chick')
elif predict[0][2] == 1.:
  print('Sapphire Gem')
elif predict[0][3] == 1.:
  print('Speckled Sussex')    
else:
  print('Wyandotte')

#Success

#save the model in pickle
pkl.dump(model, open('model.pkl', 'wb'))

pickled_model = pkl.load(open('model.pkl', 'rb'))

#try to predict
upload_file = files.upload()
 
for file_name in upload_file.keys():
 
  # predicting images
  file_path = file_name
  img = image.load_img(file_path, target_size=(128,128))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)
 
  images = np.vstack([x])
  predict = pickled_model.predict(images, batch_size=10)
  print(file_name)
  predict

#try the prediction
if predict[0][0] == 1.:
  print('American Gamefowl')
elif predict[0][1] == 1.:
  print('Chick')
elif predict[0][2] == 1.:
  print('Sapphire Gem')
elif predict[0][3] == 1.:
  print('Speckled Sussex')    
else:
  print('Wyandotte')

